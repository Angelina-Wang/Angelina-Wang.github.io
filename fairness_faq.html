<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- Google tag (gtag.js) -->
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-SC6X1YW1BH"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-SC6X1YW1BH');
    </script>
    
    <script async="" src="./website_files/js"></script>
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:400italic,600italic,700italic,400,600,700" rel="stylesheet" type="text/css">

    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Angelina Wang</title>

    <link href="./website_files/css" rel="stylesheet" type="text/css">
    <link href="./website_files/bootstrap.min.css" rel="stylesheet">
    <link href="./website_files/homepage.css" rel="stylesheet" type="text/css">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
<style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f9f9f9;
            color: #333;
        }
        .faq-container {
            max-width: 980px;
/*            margin: 20px auto 50px 0;*/
margin: 50px auto;
            padding: 20px;
        }
        .faq-item {
            border-bottom: 1px solid #ddd;
            padding: 15px 0;
        }
        .faq-item:last-child {
            border-bottom: none;
        }
        .faq-question {
            font-size: 1.2em;
            font-weight: bold;
            cursor: pointer;
            transition: color 0.3s ease;
        }
        .faq-question:hover {
            color: #007BFF;
        }
        .faq-answer {
            display: none;
            margin-top: 10px;
            font-size: 1em;
        }
    </style>

  </head>
  <body>

    <div class="container">
      <header>
        <h1>
        Fairness FAQ
      </h1>
      <a href=index.html>&larr; Home</a>
      </header>
    </div>
    <div class="faq-container">
        <p style="font-size: 1.2em">A lot more people in the ML and broader research community have been thinking about issues of algorithmic fairness and societal impact. I’ve compiled here a set of frequently asked questions that often come up when folks from primarily technical research backgrounds start thinking about this space. This is a living document, please provide any feedback <a href="https://docs.google.com/forms/d/e/1FAIpQLSf9cXATXHASjSNy93un5wMVEyHtMzT7mybZQwB2E1v-Rs5eQg/viewform?usp=sf_link">here</a>. </p>
        <div class="faq-item">
            <div class="faq-question">+ What is algorithmic fairness and why is it important?</div>
            <div class="faq-answer">Algorithmic fairness broadly refers to the disparate impact that algorithms can have towards different social groups, e.g., Black people, women, Black women. This is an important problem because <a href="https://www.technologyreview.com/2014/10/21/170679/technology-and-inequality/">technology has disproportionately benefited the more privileged over the less privileged</a>, exacerbating societal inequalities. We need to ensure that the advances we build today do not continue that trend as AI and algorithms become more prevalent in real-world settings.
            <br><br>
            For example, in the American criminal justice system, an <a hrer="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">algorithmic risk prediction tool had higher false positives</a> for Black people than White people. In the healthcare setting, Black patients assigned the same risk score for triage prioritization as White patients were <a href="https://www.science.org/doi/full/10.1126/science.aax2342">found to actually be sicker</a>. In job hiring, automated screening tools <a href="https://www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against">downranked resumes that contained the word “women,”</a> like in “women’s tennis team.”
            <br><br>
            Overall, fairness work can employ a broad range of methods and topics, for instance, quantitative evaluation, qualitative evaluation, algorithmic mitigation, and analysis of how algorithms interact with individuals. <a href="https://facctconference.org/">ACM FAccT</a> and <a href="https://www.aies-conference.com/">AAAI/ACM AIES</a> are two of the prominent top-tier fairness conference venues. As some examples of what algorithmic fairness research looks like, the following are the best paper awards from FAccT 2024:
            <ul>
                <li><a href="https://dl.acm.org/doi/10.1145/3630106.3658899">Algorithmic Pluralism: A Structural Approach To Equal Opportunity</a></li>
                <li><a href="https://dl.acm.org/doi/10.1145/3630106.3658959">Auditing Work: Exploring the New York City algorithmic bias audit regime</a></li>
                <li><a href="https://dl.acm.org/doi/10.1145/3630106.3658988">Learning about Responsible AI On-The-Job: Learning Pathways, Orientations, and Aspirations</a></li>
                <li><a href="https://dl.acm.org/doi/10.1145/3630106.3659002">Real Risks of Fake Data: Synthetic Data, Diversity-Washing and Consent Circumvention</a></li>
                <li><a href="https://dl.acm.org/doi/10.1145/3630106.3659017">Akal Badi ya Bias: An Exploratory Study of Gender Bias in Hindi Language Technology</a></li>
                <li><a href="https://dl.acm.org/doi/10.1145/3630106.3659044">Recommend Me? Designing Fairness Metrics with Providers</a></li>
            </ul>
            </div>
        </div>
        <div class="faq-item">
            <div class="faq-question">+ I just do computer science/theory/technical work, how is this relevant to me?</div>
            <div class="faq-answer">
                If you believe your research has any real-world impact, then it inherently carries the potential for social impact. Even the most theoretical science generally claims some degree of relevance to the real world, and much of technology is inherently dual-use—it can lead to outcomes that are beneficial, harmful, or somewhere in between.
                <br><br>
                For example, if you are working with facial datasets like CelebA, it’s worth considering the potential <a href="https://arxiv.org/abs/2309.15084">surveillance applications of computer vision</a>. If you are working on object recognition, whether those systems will perform well on the <a href="https://arxiv.org/abs/1906.02659">objects of countries which may look different</a> from those in the USA or Western Europe. If you are working on image captioning, whether the captioning systems trained on captions generated by sighted users (COCO dataset) will generalize to the <a href="https://arxiv.org/abs/2002.08565">different kinds of captions desired by blind users (VizWiz dataset)</a>. If you are working on motion capture systems, how incorrect assumptions about bodies <a href="https://dl.acm.org/doi/full/10.1145/3613904.3642004">fail to generalize across diverse body types</a>. And if you are working with any labeled dataset, where that dataset came from and whether participants provided <a href="https://ieeexplore.ieee.org/abstract/document/9423393">consent</a> or received proper compensation (e.g., participants have previously <a href="https://www.wsj.com/podcasts/the-journal/the-hidden-workforce-that-helped-filter-violence-and-abuse-out-of-chatgpt/ffc2427f-bdd8-47b7-9a4b-27e7267cf413">reviewed distributing content for two to three dollars an hour</a>).
                <br><br>
                While it may be easy to justify your own work as simply research, and these issues as those that arise during application, research findings have an impact on deployment practices. For instance, the best image captioning model according to research practice may be taken as the base model for a deployed application. However, this <a href="https://arxiv.org/abs/1802.08218">model may still do poorly</a> when used as an assistive technology for blind and low vision users.
                <br><br>
                That being said, of course not every ML work needs to be fairness work. However, possible fairness implications are important to be aware of, and if significant enough, described as a <a href="https://dl.acm.org/doi/abs/10.1145/3531146.3533780">broader impact</a> or <a href="https://aclrollingreview.org/cfp">limitations</a> section. This can ensure that technology is developed ethically from the start. One useful analogy here is to <a href="https://en.wikipedia.org/wiki/Secure_by_design">security by design</a> (i.e., security built in foundationally from the start) compared to bolt-on security, which is less effective. Another useful analogy is to <a href="https://en.wikipedia.org/wiki/Technical_debt">technical debt</a>, where there will be time-consuming and expensive future consequences to taking shortcuts and not doing sufficient testing now. <a href="https://link.springer.com/article/10.1007/s43681-020-00030-3">Ethical debt</a> occurs when we prioritize values now that will lead to eventual ethical consequences.
            </div>
        </div>
        <div class="faq-item">
            <div class="faq-question">+ What equation is best to measure fairness? And can we optimize for it?</div>
            <div class="faq-answer">
                There is <a href="https://www.cell.com/patterns/fulltext/S2666-3899(24)00239-3">no single measure for fairness</a>, only measurements that capture different facets of this complex construct. As an example, consider how a bank should distribute a loan. Is it fair if women and men receive the same number of loans? What if more men applied for loans than women? What if more qualified men applied for loans than qualified women? What if women are only applying for less loans because historically they were not allowed to and have been socialized to seek out life paths which require less loans? How can we consider the emotional costs incurred when someone is rejected for a loan they need to pay urgently due rent? And what about non-binary people? No single measure can capture all of this nuance, and one certainly cannot when you consider all of the domains that AI might be used in, e.g., healthcare, hiring. 
                <br><br>
                Added on to this difficulty of measurement is that machine learning outputs are rarely used directly. Generally, the output is used to <a href="https://www.annualreviews.org/content/journals/10.1146/annurev-lawsocsci-041221-023808">assist a human decision-maker</a>, so even knowing the error rate of the model is insufficient, since humans act with discretion on top of the data (when e.g., <a href="https://academic.oup.com/ser/article/17/2/283/4098110">hiring</a>, <a href="https://academic.oup.com/socpro/article/68/3/608/5782114">policing</a>). This makes it hard to optimize models at just the model-level, without engaging with how the model will be used in practice. Ultimately, fairness is a <a href="https://www.science.org/doi/10.1126/science.adi8982">sociotechnical problem</a>, and no technology-only intervention can solve it.
                <br><br>
                The above were only examples about predictive AI (i.e., forecasting future outcomes). Generative AI (i.e., creating novel content) that has multi-modal outputs can be <a href="https://arxiv.org/abs/2411.10939">more complicated</a> because of its open-ended outputs, and can lead to different manifestations of fairness-related harms, such as stereotyping. Thinking about fairness as a simple equation or quota can not only be overly simplistic but also lead to undesirable outcomes, such as <a href="https://time.com/6836153/ethical-ai-google-gemini-debacle/">racially diverse Nazis</a>. 
                <br><br>
                This may feel unsatisfying, and for those used to thinking about problems as having an “objective” answer, it can be uncomfortable to sit with ambiguity. However, a big part of fairness research is <a href="https://plato.stanford.edu/entries/feminism-epistemology/">embracing this ambiguity and partiality in the science</a>, and making progress nonetheless. Because while there is no definite way to achieve total fairness, there are still ways to characterize one model (e.g., which grants only men loans) as less fair than another (e.g., which grants comparable proportions of people from each gender loans).
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">+ What’s with the fairness impossibility theorems?</div>
            <div class="faq-answer">The fairness impossibility theorems (<a href="https://arxiv.org/abs/1610.07524">Chouldechova 2016</a>, <a href="https://arxiv.org/abs/1609.05807">Kleinberg et al. 2016</a>) demonstrate that in most real-world settings of binary classification, it is impossible to satisfy two reasonable measures of fairness (e.g., equal error rates across groups and calibrated predictions for each group) simultaneously. Some have pointed to these results as evidence that no matter what we do, fairness cannot be achieved. However, that mistakes each of the equations implicated in the impossibility theorems for a full notion of fairness: "What is strictly impossible here is the perfect balance of three specific group-based statistical measures. Labeling a particular incompatibility of statistics as an impossibility of fairness generally is mistaking the map for the territory" (<a href="https://econcs.seas.harvard.edu/files/econcs/files/green_icml18.pdf">Green and Hu 2018</a>). Instead, this tells us that algorithmic fairness is a substantively hard sociotechnical problem, where we cannot rely on just the satisfaction of various statistical criterion.</div>
        </div>

        <div class="faq-item">
            <div class="faq-question">+ When will we know that our model is sufficiently fair?</div>
            <div class="faq-answer">
                Fairness is not a static, or possible, ideal to be achieved. It will always involve <a href="https://www.cell.com/patterns/fulltext/S2666-3899(24)00239-3">trade-offs</a> that must be continuously re-negotiated. Some have argued that we should start thinking about models as simply whether they are "<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4924588">fair enough</a>," because there will always remain a reasonable fairness equation that is unsatisfied. However, we should not think about only needing algorithms to be “fair enough” as a lower threshold that allows us to put our hands up and give up on fairness. Instead, this means that we have to be even more careful in how we justify why we believe a particular model is fair enough for all of the stakeholders involved, and that enough <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4590481">alternatives were considered</a>.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">+ Isn’t model bias just a dataset bias problem though?</div>
            <div class="faq-answer">
                Imagine saying that "model accuracy is just a dataset quality problem." Of course model bias (and model accuracy) depend on dataset bias (and dataset quality), but there are <a href="https://www.sciencedirect.com/science/article/pii/S2666389921000611">many other factors at play</a>, such as <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/eb3c42ddfa16d8421fdba13528107cc1-Abstract-Conference.html">architecture</a>, loss function, <a href="https://arxiv.org/abs/1901.02547">target variable</a>. Beyond that, models can <a href="https://arxiv.org/abs/2102.12594">amplify bias</a> beyond what is in the dataset. 
                <br><br>
                Additionally, the narrative that model bias is just dataset bias is unproductive. It has often been employed by model trainers to punt the problem to a different part of the machine learning pipeline. But, there is no such thing as an unbiased dataset: even if you were to somehow collect every single image available in the world, it would still not be an unbiased look at the world because it would be mediated by what humans chose to take pictures of, and which humans were the ones taking the pictures. On top of that, the world itself is biased and we may not want to be replicating that bias with our models. So, a more productive way of thinking about the problem is taking a multi-faceted approach to not only reducing <a href="https://arxiv.org/abs/2303.06167">dataset bias</a>, but also addressing all of the other stages in a machine learning <a href="https://dl.acm.org/doi/10.1145/3465416.3483305">life cycle</a>.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">+ Perhaps a fair model would be one that doesn’t use race, or gender, or any of those attributes?</div>
            <div class="faq-answer">
                This is often called "fairness through blindness" in the community, contrasting with "<a href="https://arxiv.org/abs/1104.3913">fairness through awareness</a>." Besides the fact that it is nearly impossible to remove all possible proxies for race from a model (e.g., even zip code as an input feature encodes a lot of information about race because of the racial segregation of neighborhoods in America), this is likely not what is desired. For example, we can imagine a person who claims they “do not see race.” While there might be good intentions behind such a statement, this fails to recognize that race, gender, and other attributes have very real impacts on individuals in our society (e.g., <a href="https://www.pewresearch.org/short-reads/2023/03/01/gender-pay-gap-facts/">gender pay gap</a>, <a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC9901820/">racial discrimination through redlining</a>), and a model which fails to acknowledge this is one which is operating in a social vacuum.
                <br><br>
                <img src="files/equalityequity.png" alt="Three people of different heights watching a baseball game over a fence: under 'Equality,' they all stand on boxes of the same height, but only the tallest sees over the fence; under 'Equity,' the boxes are distributed to ensure everyone can see, with the shortest person getting more support."  width="400" >
                <br>

                Image credit: <a href="interactioninstitute.org">Interaction Institute for Social Change</a> | <a href="madewithangus.com">Artist: Angus Maguire</a>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">+ Since fairness is "sociotechnical," is it still computer science?</div>
            <div class="faq-answer">
                There is certainly a lot of fairness work that would not be considered computer science. But there is also a lot of fairness work that is computer science! In fact, <a href="https://arxiv.org/abs/1912.04883">computer science expertise is important in this space</a>. It is needed to build and evaluate AI systems to ensure they align with our desires. Without computer science expertise, we would not have sound ways to understand the workings of computer systems to measure and mitigate their fairness impact. <a href="https://arxiv.org/abs/2206.07173">Measurements</a> informed by CS expertise can interrogate the inner workings of ML models, to better direct interventions, instead of only at the output stage. Research on spurious correlations, out-of-distribution classification, robustness, are all technical directions with implications for fairness.
                <br><br>
                A related computer science subfield is <a href="https://www.designdisciplin.com/p/hci-profession">human-computer interaction</a>, which has historically sometimes had <a href="https://mcorrell.medium.com/the-othering-of-hci-ab0a07edc69f">trouble getting accepted by computer science departments</a> as sufficiently "computer science." This can be at least partially attributed to the <a href="https://www.youtube.com/watch?v=OL3DowBM9uc">hierarchies of knowledge</a> about what is valued as "difficult," "important," and "objective," knowledge.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">+ But aren’t humans just as biased as machines? At least with machines we can better scrutinize them.</div>
            <div class="faq-answer">
                This is true in certain respects, but we have to also remember the downsides of <a href="https://dl.acm.org/doi/10.1145/3636509">machine decision-making</a> compared to <a href="https://dl.acm.org/doi/10.1145/3531146.3533223">human decision-making</a>. For one, with a human decision-maker, there is often the opportunity for <a href="https://en.wikipedia.org/wiki/Street-level_bureaucracy">discretion</a> and judgment that can account for context (e.g., a traffic accident that caused someone to miss their appointment time) or errors (e.g., a typo on a name form). Of course, this discretion comes with its own risks of bias and inconsistencies.  While algorithms can be designed to permit more leniency, in practice, they generally follow rigid rules without adequate intervention and contestability mechanisms. Second, there’s a form of <a href="https://en.wikipedia.org/wiki/Automation_bias">automation bias</a> where people believe that decisions from machines are both more accurate and more objective, regardless of whether that’s true. Third, human decision-makers can be held accountable for their actions and learn from their mistakes in a more dynamic way than model updates. And a fourth reason is that ML systems scale in a dramatic way compared to human decision-makers. For instance, in the Netherlands a predictive algorithm for detecting welfare fraud <a href="https://www.politico.eu/article/dutch-scandal-serves-as-a-warning-for-europe-over-risks-of-using-algorithms/">wrongly accused</a> 26,000 parents of welfare fraud, driving families into debt. Another externality of this scale is how widespread the harms can be. While individual humans may be more or less biased in different ways such that the noise of one decision-maker (e.g., a hiring manager at Company A) can be different from the noise of another (e.g., a hiring manager at Company B), one biased ML model can be rapidly deployed across a wide range of settings and enact the <a href="https://www.cambridge.org/core/journals/canadian-journal-of-philosophy/article/algorithmic-leviathan-arbitrariness-fairness-and-opportunity-in-algorithmic-decisionmaking-systems/3AA0ECA77F8622488E9DB0834287215B">same bias against the same individuals</a>. These four factors as well as others make the calculus more complicated than the question initially suggests. Whether a human or machine or both in conjunction are better to be deployed in a particular application requires navigating many trade-offs and, as is a common refrain in fairness, “depends on the application.”
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">+ I’m a fairness researcher. These answers feel a bit simplistic.</div>
            <div class="faq-answer">
            Yes, this is only an introduction, and at times compromised on nuance for clarity—and to not sound too preachy. If you have any feedback, or examples you have felt better communicate these concepts, please submit them <a href="https://docs.google.com/forms/d/e/1FAIpQLSf9cXATXHASjSNy93un5wMVEyHtMzT7mybZQwB2E1v-Rs5eQg/viewform?usp=sf_link">here</a>!</div>
        </div>

        <div class="faq-item">
            <div class="faq-question">+ Where can I learn more?</div>
            <div class="faq-answer">
                The following books are great places to start:
                <ul>
                    <li><a href="https://www.ruhabenjamin.com/race-after-technology">Race After Technology</a> by Ruha Benjamin</li>
                    <li><a href="https://nyupress.org/9781479837243/algorithms-of-oppression/">Algorithms of Oppression</a> by Safiya Noble</li>
                    <li><a href="https://us.macmillan.com/books/9781250074317/automatinginequality">Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor</a> by Virginia Eubanks</li>
                    <li><a href="https://www.penguinrandomhouse.com/books/241363/weapons-of-math-destruction-by-cathy-oneil/">Weapons of Math Destruction</a> by Cathy O'Neil</li>
                    <li><a href="https://mitpress.mit.edu/9780262547185/data-feminism/">Data Feminism</a> by Catherine D'Ignazio and Lauren F. Klein (or the shorter <a href="https://dl.acm.org/doi/10.1145/3630106.3658543">Data Feminism for AI</a>) </li>
                </ul>
            </div>
        </div>
        <br>
        <p>This is inspired by Sanmi Koyejo and Olga Russakovsky’s CVPR 2022 tutorial <a href="https://sites.google.com/view/cvpr2022-fairness-tutorial">Algorithmic fairness: why it’s hard and why it’s interesting</a>. That tutorial is also a <strong>fantastic</strong> place to learn more. I am grateful to feedback from Anastasia Koloskova, Sanmi Koyejo, Olga Russakovsky, Brent Yi.
</p>
    </div>

<script>

        document.querySelectorAll('.faq-question').forEach(question => {
            question.addEventListener('click', () => {
                const answer = question.nextElementSibling;
                answer.style.display = answer.style.display === 'block' ? 'none' : 'block';
            });
        });

    </script>
    <script src="./website_files/bootstrap.min.js"></script>
  

</body></html>